{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from heapq import nlargest\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сумаризація тексту за допомогою `nltk`\n",
    "\n",
    "1. [Суммаризация текста: подходы, алгоритмы, рекомендации и перспективы](https://habr.com/ru/articles/514540/)\n",
    "\n",
    "Скористаємось алгоритмом екстрактивної сумаризації запропонованим в статті [1]\n",
    "\n",
    "- Розбиття вхідного тексту на окремі речення \n",
    "- Переведення речення у цифрове представлення (вектор).\n",
    "- Обчислення і збереження в матриці подібності подібності між векторами речень.\n",
    "- Перетворення отриманої матриці на граф із реченнями у вигляді вершин і оцінками подібності у вигляді ребер для обчислення рангу речень.\n",
    "- Вибір пропозицій з найвищою оцінкою для підсумкового резюме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Створюємо теку з українськими стоп-словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Перевіряємо наявність папки corpora/stopwords у каталозі nltk_data\n",
    "nltk_data_path = nltk.data.path[0]\n",
    "stopwords_path = os.path.join(nltk_data_path, \"corpora\", \"stopwords\")\n",
    "if not os.path.exists(stopwords_path):\n",
    "    os.makedirs(stopwords_path)\n",
    "\n",
    "\n",
    "# Завантажуємо стоп-слова для української мови та зберігаємо у файл\n",
    "url = \"https://raw.githubusercontent.com/skupriienko/Ukrainian-Stopwords/master/stopwords_ua.txt\"\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(os.path.join(stopwords_path, \"ukrainian\"), \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"ukrainian\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завантажуємо текст з файлу у змінну"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_ua.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визначаємо функції\n",
    "\n",
    "1. [Cosine Similarity and Cosine Distance](https://medium.com/geekculture/cosine-similarity-and-cosine-distance-48eed889a5c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    \"\"\"\n",
    "    Функція вимірює схожість між двома реченнями. Використовується косинусна\n",
    "    відстань між векторами, що представляють слова в реченнях. Вона також\n",
    "    враховує стоп-слова.\n",
    "    \"\"\"\n",
    "    if stopwords is None:\n",
    "        stopwords = set()\n",
    "\n",
    "    # Токенізуємо речення та видаляємо стоп-слова\n",
    "\n",
    "    words1 = [\n",
    "        word.lower()\n",
    "        for word in word_tokenize(sent1)\n",
    "        if word.isalnum() and word.lower() not in stopwords\n",
    "    ]  # створюємо список слів (без стоп-слів) із першого речення\n",
    "    words2 = [\n",
    "        word.lower()\n",
    "        for word in word_tokenize(sent2)\n",
    "        if word.isalnum() and word.lower() not in stopwords\n",
    "    ]  # створюємо список слів (без стоп-слів) із першого речення\n",
    "\n",
    "    all_words = list(\n",
    "        set(words1 + words2)\n",
    "    )  # створюємо список унікальних слів з двох речень\n",
    "\n",
    "    # Створюємо вектори типу компонентами яких є  одиниці та нулі\n",
    "    # якщо слово є в речення, то ставимо 1, інакше 0\n",
    "    # Отримуємо щось таке [1, 0, 0, 1, ...]\n",
    "    vector1 = [1 if word in words1 else 0 for word in all_words]\n",
    "    vector2 = [1 if word in words2 else 0 for word in all_words]\n",
    "\n",
    "    # розраховуємо косинусну відстань між двома векторами\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    \"\"\"\n",
    "    Функція будує матрицю схожості між усіма парами речень у тексті.\n",
    "    \"\"\"\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = sentence_similarity(\n",
    "                    sentences[i], sentences[j], stop_words\n",
    "                )\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(text, num_sentences=3, stop_words=stop_words):\n",
    "    \"\"\"\n",
    "    Функція створює короткий зміст тексту.\n",
    "\n",
    "    Алгоритм роботи:\n",
    "\n",
    "    1. Читаємо речення із тексту і потім будуємо матрицю схожості.\n",
    "    2. Розраховуємо суму схожості для кожного речення.\n",
    "    3. Використовуємо nlargest для вибору топ N речень за порядком зменшення схожості.\n",
    "    4. Об'єднуємо вибрані речення в один рядок.\n",
    "\n",
    "    \"\"\"\n",
    "    summarize_text = []\n",
    "\n",
    "    sentences = sent_tokenize(text)  # Токенізуємо текст\n",
    "\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    sentence_similarity_scores = np.array(\n",
    "        [sum(row) for row in sentence_similarity_matrix]\n",
    "    )\n",
    "\n",
    "    # Використовуємо nlargest для вибору топ N речень за порядком зменшення схожості\n",
    "    top_sentences_indices = nlargest(\n",
    "        num_sentences,\n",
    "        range(len(sentence_similarity_scores)),\n",
    "        key=sentence_similarity_scores.__getitem__,\n",
    "    )\n",
    "\n",
    "    # Формуємо короткий зміст з вибраних речень\n",
    "    for i in top_sentences_indices:\n",
    "        summarize_text.append(sentences[i])\n",
    "\n",
    "    return \" \".join(summarize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Записуємо анотований текст до файлу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 3\n",
    "stop_words = set(stopwords.words(\"ukrainian\"))\n",
    "summary = generate_summary(text, num_sentences, stop_words)\n",
    "with open(\"summary_ua.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Висновки\n",
    "\n",
    "Отже, ми маємо [вихідний текст](./text_ua.txt):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Орбітальний корабель \"Діскавері\", OV-103, вважається таким, що має право на внесення до Національного реєстру історичних місць (NRHP) в контексті Програми космічних човників США (1969-2011) за Критерієм А в галузі космічних досліджень і транспорту та за Критерієм С в галузі інженерії. Оскільки вона досягла значущості протягом останніх п'ятдесяти років, застосовується Розгляд критерію G. Згідно з Критерієм А, \"Діскавері\" є важливим як найстаріший з трьох збережених орбітальних кораблів, побудованих для програми \"Спейс Шаттл\" (SSP), найдовшої американської космічної програми на сьогоднішній день; він був третім з п'яти орбітальних кораблів, побудованих НАСА. На відміну від програм \"Меркурій\", \"Джеміні\" та \"Аполлон\", основна увага в SSP приділялася економічній ефективності та багаторазовому використанню, а в перспективі - створенню космічної станції. Включаючи свій перший політ (запущений 30 серпня 1984 року), \"Діскавері\" здійснив тридцять дев'ять польотів у космос, більше, ніж будь-який з інших чотирьох орбітальних апаратів; він також став першим орбітальним апаратом, який здійснив двадцять місій. Вона мала честь бути обраною для повернення до польотів після аварій \"Челленджера\" і \"Колумбії\". \"Діскавері\" став першим шатлом, який здійснив політ з модернізованими SRB після аварії \"Челленджера\", і першим шатлом, який здійснив політ з SSME Фази II і Блоку I. \"Діскавері\" також доставив на орбіту космічний телескоп \"Габбл\" і здійснив дві з п'яти місій з обслуговування обсерваторії. Вона виконала першу і останню місії Міністерства оборони США, а також першу несекретну місію, пов'язану з обороною. Крім того, \"Діскавері\" відіграв важливу роль у будівництві Міжнародної космічної станції (МКС); він здійснив тринадцять з тридцяти семи польотів на станцію за допомогою американського космічного човника. Вона була першим орбітальним кораблем, який пристикувався до МКС, і першим, який здійснив обмін екіпажем, що проживає на станції. За критерієм С \"Діскавері\" є важливим як інженерний подвиг. За словами Вейна Хейла, керівника польотів Космічного центру імені Джонсона, орбітальний корабель \"Спейс Шаттл\" являє собою \"величезний технологічний стрибок від одноразових ракет і капсул до багаторазового, крилатого, гіперзвукового, вантажного космічного корабля\". Хоча його базова конструкція наслідує конструкцію звичайного літака, він використовує передові матеріали, які мінімізують його вагу для перевезення вантажу і мають низький коефіцієнт теплового розширення, що забезпечує стабільну основу для матеріалів системи теплового захисту (TPS). На орбітальному кораблі \"Спейс Шаттл\" також була встановлена перша багаторазова система TPS; всі попередні космічні кораблі мали одноразовий абляційний теплозахисний екран. Серед інших визначних інженерних досягнень орбітального корабля - перша багаторазова орбітальна рухова установка і перша стійка до двох відмов інтегрована система авіоніки. За словами Хейла, \"Спейс Шаттл\" залишається \"найбільшим, найшвидшим, найкрилатішим гіперзвуковим літаком в історії\", який регулярно літає зі швидкістю, що у двадцять п'ять разів перевищує швидкість звуку."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "та [його анотацію](./summary_ua.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Включаючи свій перший політ (запущений 30 серпня 1984 року), \"Діскавері\" здійснив тридцять дев'ять польотів у космос, більше, ніж будь-який з інших чотирьох орбітальних апаратів; він також став першим орбітальним апаратом, який здійснив двадцять місій. Крім того, \"Діскавері\" відіграв важливу роль у будівництві Міжнародної космічної станції (МКС); він здійснив тринадцять з тридцяти семи польотів на станцію за допомогою американського космічного човника. \"Діскавері\" став першим шатлом, який здійснив політ з модернізованими SRB після аварії \"Челленджера\", і першим шатлом, який здійснив політ з SSME Фази II і Блоку I."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Довжина вихідного тексту: 3110\n",
      "Довжина анотації: 621\n"
     ]
    }
   ],
   "source": [
    "print(f\"Довжина вихідного тексту: {len(text)}\")\n",
    "print(f\"Довжина анотації: {len(summary)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ту ж саму задачу я поставив `chatGPT`, ось його результат:\n",
    "\n",
    "\"Орбітальний корабель 'Діскавері' (OV-103) визнається історично значущим за Критеріями А та С в контексті Програми космічних човників США, як найстарший збережений орбітальний апарат, здійснивши 39 польотів, включаючи місії обслуговування телескопу 'Габбл' та важливий внесок у будівництво Міжнародної космічної станції, а також вперше використовуючи багаторазову систему теплового захисту та інші інновації в інженерії за останні п'ятдесят років.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
