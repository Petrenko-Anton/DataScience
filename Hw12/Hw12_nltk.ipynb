{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from heapq import nlargest\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sergiy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sergiy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сумаризація тексту за допомогою `nltk`\n",
    "\n",
    "1. [Суммаризация текста: подходы, алгоритмы, рекомендации и перспективы](https://habr.com/ru/articles/514540/)\n",
    "\n",
    "Скористаємось алгоритмом екстрактивної сумаризації запропонованим в статті [1]\n",
    "\n",
    "- Розбиття вхідного тексту на окремі речення \n",
    "- Переведення речення у цифрове представлення (вектор).\n",
    "- Обчислення і збереження в матриці подібності подібності між векторами речень.\n",
    "- Перетворення отриманої матриці на граф із реченнями у вигляді вершин і оцінками подібності у вигляді ребер для обчислення рангу речень.\n",
    "- Вибір пропозицій з найвищою оцінкою для підсумкового резюме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Створюємо теку з українськими стоп-словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Перевіряємо наявність папки corpora/stopwords у каталозі nltk_data\n",
    "nltk_data_path = nltk.data.path[0]\n",
    "stopwords_path = os.path.join(nltk_data_path, \"corpora\", \"stopwords\")\n",
    "if not os.path.exists(stopwords_path):\n",
    "    os.makedirs(stopwords_path)\n",
    "\n",
    "# Завантажуємо стоп-слова для української мови та зберігаємо у файл\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/olegdubetcky/Ukrainian-Stopwords/main/ukrainian\"\n",
    ")\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(os.path.join(stopwords_path, \"ukrainian\"), \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Додаємо українські стоп-слова в NLTK\n",
    "stop_words = set(stopwords.words(\"ukrainian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завантажуємо текст з файлу у змінну"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_ua.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визначаємо функції\n",
    "\n",
    "1. [Cosine Similarity and Cosine Distance](https://medium.com/geekculture/cosine-similarity-and-cosine-distance-48eed889a5c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    \"\"\"\n",
    "    Функція вимірює схожість між двома реченнями. Використовується косинусна відстань між векторами, що представляють слова в реченнях. Вона також враховує стоп-слова.\n",
    "    \"\"\"\n",
    "    if stopwords is None:\n",
    "        stopwords = set()\n",
    "\n",
    "    # Токенізуємо речення та видаляємо стоп-слова\n",
    "\n",
    "    words1 = [\n",
    "        word.lower()\n",
    "        for word in word_tokenize(sent1)\n",
    "        if word.isalnum() and word.lower() not in stopwords\n",
    "    ]  # створюємо список слів (без стоп-слів) із першого речення\n",
    "    words2 = [\n",
    "        word.lower()\n",
    "        for word in word_tokenize(sent2)\n",
    "        if word.isalnum() and word.lower() not in stopwords\n",
    "    ]  # створюємо список слів (без стоп-слів) із першого речення\n",
    "\n",
    "    all_words = list(\n",
    "        set(words1 + words2)\n",
    "    )  # створюємо список унікальних слів з двох речень\n",
    "\n",
    "    # Створюємо вектори типу компонентами яких є  одиниці та нулі\n",
    "    # якщо слово є в речення, то ставимо 1, інакше 0\n",
    "    # Отримуємо щось таке [1, 0, 0, 1, ...]\n",
    "    vector1 = [1 if word in words1 else 0 for word in all_words]\n",
    "    vector2 = [1 if word in words2 else 0 for word in all_words]\n",
    "\n",
    "    # розраховуємо косинусну відстань між двома векторами\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    \"\"\"\n",
    "    Функція будує матрицю схожості між усіма парами речень у тексті.\n",
    "    \"\"\"\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = sentence_similarity(\n",
    "                    sentences[i], sentences[j], stop_words\n",
    "                )\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(text, num_sentences=3, stop_words=stop_words):\n",
    "    \"\"\"\n",
    "    Функція створює короткий зміст тексту.\n",
    "\n",
    "    Алгоритм роботи:\n",
    "\n",
    "    1. Читаємо речення із тексту і потім будуємо матрицю схожості.\n",
    "    2. Розраховуємо суму схожості для кожного речення.\n",
    "    3. Використовуємо nlargest для вибору топ N речень за порядком зменшення схожості.\n",
    "    4. Об'єднуємо вибрані речення в один рядок.\n",
    "\n",
    "    \"\"\"\n",
    "    summarize_text = []\n",
    "\n",
    "    sentences = sent_tokenize(text)  # Токенізуємо текст\n",
    "\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    sentence_similarity_scores = np.array(\n",
    "        [sum(row) for row in sentence_similarity_matrix]\n",
    "    )\n",
    "\n",
    "    # Використовуємо nlargest для вибору топ N речень за порядком зменшення схожості\n",
    "    top_sentences_indices = nlargest(\n",
    "        num_sentences,\n",
    "        range(len(sentence_similarity_scores)),\n",
    "        key=sentence_similarity_scores.__getitem__,\n",
    "    )\n",
    "\n",
    "    # Формуємо короткий зміст з вибраних речень\n",
    "    for i in top_sentences_indices:\n",
    "        summarize_text.append(sentences[i])\n",
    "\n",
    "    return \" \".join(summarize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Записуємо анотований текст до файлу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 1\n",
    "stop_words = set(stopwords.words(\"ukrainian\"))\n",
    "summary = generate_summary(text, num_sentences, stop_words)\n",
    "with open(\"summary_ua.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Висновки\n",
    "\n",
    "1. [Вихідний текст](./text_ua.txt)\n",
    "2. [Анотація](./summary_ua.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Довжина вихідного тексту: 3110\n",
      "Довжина анотації: 252\n"
     ]
    }
   ],
   "source": [
    "print(f\"Довжина вихідного тексту: {len(text)}\")\n",
    "print(f\"Довжина анотації: {len(summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таким чином текст довжиною 3110 слів зменшився до 252 слів для 1 речень.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Таким чином текст довжиною {len(text)} слів зменшився до {len(summary)} слів для {num_sentences} речень.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
