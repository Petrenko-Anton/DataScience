{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анотування тексту за допомогою `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sergiy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sergiy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Створюємо теку з українськими стоп-словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Перевіряємо наявність папки corpora/stopwords у каталозі nltk_data\n",
    "nltk_data_path = nltk.data.path[0]\n",
    "stopwords_path = os.path.join(nltk_data_path, \"corpora\", \"stopwords\")\n",
    "if not os.path.exists(stopwords_path):\n",
    "    os.makedirs(stopwords_path)\n",
    "\n",
    "# Загружаем стоп-слова для украинского языка и сохраняем в файл\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/olegdubetcky/Ukrainian-Stopwords/main/ukrainian\"\n",
    ")\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(os.path.join(stopwords_path, \"ukrainian\"), \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Добавляем украинские стоп-слова в NLTK\n",
    "stop_words = set(stopwords.words(\"ukrainian\"))\n",
    "with open(os.path.join(stopwords_path, \"ukrainian\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Объединяем стандартные и кастомные стоп-слова\n",
    "stop_words.update(custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завантажуємо текст з файлу у змінну"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_ua.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визначаємо функції"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(text):\n",
    "    \"\"\"\n",
    "    Функція використовується для поділу тексту на речення за допомогою nltk.sent_tokenize.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    \"\"\"\n",
    "    Функція вимірює схожість між двома реченнями. Використовується косинусна відстань між векторами, що представляють слова в реченнях. Вона також враховує стоп-слова.\n",
    "    \"\"\"\n",
    "    if stopwords is None:\n",
    "        stopwords = set()\n",
    "\n",
    "    words1 = [\n",
    "        word.lower()\n",
    "        for word in word_tokenize(sent1)\n",
    "        if word.isalnum() and word.lower() not in stopwords\n",
    "    ]\n",
    "    words2 = [\n",
    "        word.lower()\n",
    "        for word in word_tokenize(sent2)\n",
    "        if word.isalnum() and word.lower() not in stopwords\n",
    "    ]\n",
    "\n",
    "    all_words = list(set(words1 + words2))\n",
    "\n",
    "    vector1 = [1 if word in words1 else 0 for word in all_words]\n",
    "    vector2 = [1 if word in words2 else 0 for word in all_words]\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    \"\"\"\n",
    "    Функція будує матрицю схожості між усіма парами речень у тексті.\n",
    "    \"\"\"\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = sentence_similarity(\n",
    "                    sentences[i], sentences[j], stop_words\n",
    "                )\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(text, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Функція створює короткий зміст тексту.\n",
    "\n",
    "    Алгоритм роботи:\n",
    "\n",
    "    1. Читаємо речення із тексту і потім будуємо матрицю схожості.\n",
    "    2. Розраховуємо суму схожості для кожного речення.\n",
    "    3. Речення сортуються в порядку спадання цієї суми.\n",
    "    4. Вибибираємо топ N речень (де N - кількість речень у стислому змісті) за порядком зменшення схожості й об'єднуємо їх в один рядок.\n",
    "\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"ukrainian\"))\n",
    "    summarize_text = []\n",
    "\n",
    "    sentences = read_text(text)\n",
    "\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    sentence_similarity_scores = np.array(\n",
    "        [sum(row) for row in sentence_similarity_matrix]\n",
    "    )\n",
    "\n",
    "    ranked_sentences = [\n",
    "        sentence\n",
    "        for _, sentence in sorted(\n",
    "            zip(sentence_similarity_scores, sentences), reverse=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for i in range(min(num_sentences, len(ranked_sentences))):\n",
    "        summarize_text.append(ranked_sentences[i])\n",
    "\n",
    "    return \" \".join(summarize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Записуємо анотований текст до файлу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = generate_summary(text)\n",
    "with open(\"summary_ua.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Висновки\n",
    "\n",
    "1. [Вихідний текст](./text_ua.txt)\n",
    "2. [Анотація](./summary_ua.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Довжина вихідного тексту: 3110\n",
      "Довжина анотації: 580\n"
     ]
    }
   ],
   "source": [
    "print(f\"Довжина вихідного тексту: {len(text)}\")\n",
    "print(f\"Довжина анотації: {len(summary)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким чином текст довжиною `3110` слів зменшився до `580` слів."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
