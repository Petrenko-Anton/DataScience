{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import ConvexHull\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теорія"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Означення"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластерний аналіз (англ. Data clustering) — задача розбиття заданої вибірки об'єктів (ситуацій) на підмножини, які називаються кластерами, так, щоб кожен кластер складався зі схожих об'єктів, а об'єкти різних кластерів істотно відрізнялися. Задача кластеризації належить до статистичної обробки, а також до широкого класу завдань некерованого навчання."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластериза́ція ме́тодом k-сере́дніх (англ. k-means clustering) - впорядкування множини об'єктів у порівняно однорідні групи.\n",
    "\n",
    "Мета методу - розділити $n$ спостережень на $k$ кластерів, так щоб кожне спостереження належало до кластера з найближчим до нього середнім значенням. Метод базується на мінімізації суми квадратів відстаней між кожним спостереженням та центром його кластера, тобто функції:\n",
    "\n",
    "\\begin{equation*}\n",
    "    J = \\sum_{i=1}^{n} \\sum_{j=1}^{k} (x_i - с_j)^2\n",
    "\\end{equation*}\n",
    "\n",
    "де $k$ — число кластерів, $n$ — число спостережень, $x_i$ - $i$-те спостереження, $c_j$ - центри мас $j$-го кластера.\n",
    "\n",
    "Фунцкція $J$ - це функція втрат, котру для кластеризації ще називають `distortion`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм методу «Кластеризація за схемою k-середніх»:\n",
    "\n",
    "Маємо масив спостережень (об'єктів), кожен з яких має певні значення за рядом ознак. Відповідно до цих значень об'єкт розташовується у багатовимірному просторі.\n",
    "\n",
    "1. Дослідник визначає кількість кластерів, що необхідно утворити.\n",
    "2. Випадковим чином обирається $n$ спостережень, які на цьому кроці вважаються центрами кластерів.\n",
    "3. Кожне спостереження «приписується» до одного з $k$ кластерів — того, відстань до якого найкоротша.\n",
    "4. Розраховується новий центр кожного кластера як елемент, ознаки якого розраховуються як середнє арифметичне ознак об'єктів, що входять у цей кластер.\n",
    "5. Відбувається така кількість ітерацій (повторюються кроки 3-4), поки кластерні центри стануть стійкими (тобто при кожній ітерації в кожен кластер потрапляють одні й ті самі об'єкти), дисперсія всередині кластера буде мінімізована, а між кластерами — максимізована.\n",
    "\n",
    "Вибір кількості кластерів робиться на основі дослідницької гіпотези. Якщо її немає, то рекомендують спочатку створити 2 кластери, далі 3, 4, 5, порівнюючи отримані результати."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод \"ліктя\"\n",
    "\n",
    "Метод ліктя передбачає багаторазове циклічне виконання алгоритму зі збільшенням кількості кластерів, а також подальшим відкладанням на графіку функції втрат (`distortion`).\n",
    "\n",
    "Характерный график выглядит так:\n",
    "\n",
    "![Alt text](image/kmean.png)\n",
    "\n",
    "Графік \"ліктя\" показує залежність функції втрат $J$ від кількості кластерів $k$. Якщо на графіку можна виокремити точку згину (так званий \"лікоть\"), то це може свідчити про те, що відповідна кількість кластерів є оптимальною."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посилання\n",
    "\n",
    "1. [Документация skikit-learn. 2.3. Кластеризация](https://scikit-learn.ru/clustering/#)\n",
    "2. [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "3. [Алгоритм кластеризации Ллойда (K-средних, K-means)](https://www.youtube.com/watch?v=8vCuR1AndH0)\n",
    "4. [Модель кластеризации KMeans](https://www.youtube.com/watch?v=EHZJMz6zyFE&ab_channel=machinelearrrning)\n",
    "5. [Метод локтя для модели KMeans с нуля](https://youtu.be/BEhLlqkL-f4)\n",
    "6. [Кластеризуем лучше, чем «метод локтя»](https://habr.com/ru/companies/jetinfosystems/articles/467745/)\n",
    "7. [Кластерный анализ](https://www.dmitrymakarov.ru/intro/clustering-16/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функції"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функція для пошуку кластерів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(features, max_clusters, random_state=0) -> list:\n",
    "    \"\"\"\n",
    "    Find the optimal number of clusters using the 'Elbow Method' for K-Means clustering.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): The input data for clustering.\n",
    "        max_clusters (int): The maximum number of clusters to consider.\n",
    "\n",
    "    Returns:\n",
    "        distortions: list of distorsions\n",
    "\n",
    "    This function calculates the distortion (inertia) for different numbers of clusters\n",
    "    ranging from 1 to max_clusters and plots a 'Elbow Method' graph to help choose the\n",
    "    optimal number of clusters for K-Means clustering. The point where the distortion\n",
    "    starts to decrease at a slower rate often indicates the optimal number of clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    distortions = []\n",
    "    for i in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=i,\n",
    "            init=\"k-means++\",\n",
    "            max_iter=100,\n",
    "            n_init=\"auto\",\n",
    "            random_state=random_state,\n",
    "        ).fit(features)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    # plotting\n",
    "    plt.plot(range(1, max_clusters + 1), distortions, marker='D', color='b')\n",
    "    plt.plot(range(1, max_clusters + 1), distortions, marker='D', color='b')\n",
    "    plt.xticks(range(1, max_clusters + 1), rotation=45)\n",
    "    plt.xlabel(r\"Number of clusters, $k$\")\n",
    "    plt.ylabel(r\"Loss function (distortion)\")\n",
    "    plt.title(\"Elbow chart\")\n",
    "    plt.grid(True, linewidth=1, color='lightgray')\n",
    "    plt.show()\n",
    "\n",
    "    return distortions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функція для виконання алгоритму кластеризації за методом K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(features, n_clusters=8, random_state=0):\n",
    "    \"\"\"\n",
    "    Performs clustering using the K-Means method and returns cluster labels and centroid coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - features: The array of features to cluster.\n",
    "    - n_clusters: Number of clusters (default is 8).\n",
    "    - random_state: The seed for generating random numbers (default 0).\n",
    "\n",
    "    Returns:\n",
    "    - cluster_labels: An array of cluster labels.\n",
    "    - centroids: The coordinates of the centroids of the clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a K-Means object\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        init=\"k-means++\",\n",
    "        max_iter=100,\n",
    "        n_init=\"auto\",\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "    # Clustering\n",
    "    kmeans.fit(features)\n",
    "\n",
    "    # Get cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Пget the coordinates of centroids\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    return cluster_labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функція для візуалізації кластерів з опуклими оболонками та центроїдами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_with_convex_hulls(X, cluster_labels, centroids, k):\n",
    "    \"\"\"\n",
    "    Function to visualize clusters with convex shells and centroids.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Data matrix (n_samples, n_features).\n",
    "    - y: Class Tags.\n",
    "    - cluster_labels: Array with cluster labels for each point in X.\n",
    "    - centroids: Matrix with the coordinates of the centroids of the clusters (k, n_features).\n",
    "    - k: The number of clusters.\n",
    "\n",
    "    Result:\n",
    "    - Visualization with clusters, convex hulls and centroids.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    cmap = plt.get_cmap(\"inferno\")\n",
    "\n",
    "    hull_list = [ConvexHull(X[cluster_labels == i]) for i in range(k)]\n",
    "\n",
    "    for i, hull in enumerate(hull_list):\n",
    "        cluster_points = X[cluster_labels == i]\n",
    "        cluster_color = cmap(i / k)\n",
    "        plt.scatter(\n",
    "            cluster_points[:, 0],\n",
    "            cluster_points[:, 1],\n",
    "            s=10,\n",
    "            color=cluster_color,\n",
    "            label=f\"Cluster {i+1}\",\n",
    "        )\n",
    "\n",
    "        for simplex in hull.simplices:\n",
    "            plt.plot(\n",
    "                cluster_points[simplex, 0],\n",
    "                cluster_points[simplex, 1],\n",
    "                color=cluster_color,\n",
    "                linewidth=1,\n",
    "                linestyle=\"--\",\n",
    "            )\n",
    "\n",
    "        ax.fill(\n",
    "            cluster_points[hull.vertices, 0],\n",
    "            cluster_points[hull.vertices, 1],\n",
    "            facecolor=cluster_color,\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], s=75, marker=\"x\", c=\"r\")\n",
    "    ax.set_facecolor(\"#f5f5f5\")\n",
    "    plt.xlabel(\"component 1\")\n",
    "    plt.ylabel(\"component 2\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Двовимірний датасет `data_2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2d = pd.read_csv(\"data/data_2d.csv\", header=None)\n",
    "data_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Цей набір даних складається з 200 рядків і 3 стовпців. Перший стовпчик - це клас, який може набувати лише двох значень: $0$ або $1$. Другий стовпець - це перша ознака, яка є дійсним числом. Третій стовпець - це друга ознака, яка також є дійсним числом. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Побудуємо розподіл даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Ознака 1 - Графік \"ящик із вусами\"\n",
    "ax1.boxplot(\n",
    "    data_2d[1],\n",
    "    vert=False,\n",
    "    widths=0.7,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor=\"lightblue\"),\n",
    "    medianprops={\"color\": \"red\"},\n",
    ")\n",
    "ax1.set_title(\"Ознака 1\")\n",
    "ax1.set_yticks([])\n",
    "ax1.spines[\"left\"].set_visible(False)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Ознака 2 - Графік \"ящик із вусами\"\n",
    "ax2.boxplot(\n",
    "    data_2d[2],\n",
    "    vert=False,\n",
    "    widths=0.7,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor=\"lightcoral\"),\n",
    "    medianprops={\"color\": \"blue\"},\n",
    ")\n",
    "ax2.set_title(\"Ознака 2\")\n",
    "ax2.set_yticks([])\n",
    "ax2.spines[\"left\"].set_visible(False)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Діаграма розсіювання\n",
    "scatter = ax3.scatter(\n",
    "    data_2d[1], data_2d[2], c=data_2d[0], s=30, cmap=\"RdYlBu\", alpha=0.75\n",
    ")\n",
    "ax3.set_title(\"Scatter plot\")\n",
    "ax3.set_xlabel(\"Feature 1\")\n",
    "ax3.set_ylabel(\"Feature 2\")\n",
    "ax3.legend(*scatter.legend_elements(), title=\"Класи\", loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Додамо кольорові фони для кожного графіка\n",
    "ax1.set_facecolor(\"#f5f5f5\")\n",
    "ax2.set_facecolor(\"#f5f5f5\")\n",
    "ax3.set_facecolor(\"#f5f5f5\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Розрахунок кореляції\n",
    "corr = np.corrcoef(data_2d[1], data_2d[2])\n",
    "print(corr[0, 1].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основі графіків та коефіцієнта кореляції можна зробити висновки про дані:\n",
    "- Розподіл першої ознаки: нормальний.\n",
    "- Розподіл другої ознаки: нормальний.\n",
    "- Кореляція між ознаками: слабка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Знаходження оптимальної кількості кластерів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_2d = np.array(data_2d.iloc[:, 1:])\n",
    "y_data_2d = np.array(data_2d.iloc[:, 0])\n",
    "\n",
    "Jk_data_2d = find_optimal_clusters(X_data_2d, max_clusters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розрахунок оптимального числа кластерів здійснимо за формулою:\n",
    "\\begin{equation*}\n",
    "    k_\\mathrm{opt} = \\mathrm{argmin}\\left(\\frac{J_{k+1} - J_{k}}{J_{k} - J_{k-1}}\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.diff(Jk_data_2d)\n",
    "D = diff[1:] / diff[:-1]\n",
    "plt.plot(np.arange(2, len(D) + 2), D)\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\frac{J_{k+1} - J_{k}}{J_{k} - J_{k-1}}$')\n",
    "plt.show()\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_opt = np.argmin(D) + 2\n",
    "print(k_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судячи з графіку число кластерів для `data_2d` дорівнює $2$, оскільки мінімальне значення величини $\\frac{J_{k+1} - J_{k}}{J_{k} - J_{k-1}}$ при $k = 2$. Крім того в наших даних перша колонка скоріше за все є класом ознаки. Таких ознак в даних так $2$-і."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автоматичне знаходження кластерів\n",
    "\n",
    "Число кластерів методом \"ліктя\" також можна знайти автоматичн, використовуючи бібліотеку [`yellowbrick`](https://www.scikit-yb.org/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kelbow_data_2d = kelbow_visualizer(\n",
    "    KMeans(random_state=0, n_init=\"auto\"), X_data_2d, k=(1, 11), timings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як видно з графіку, методи бібліотеки також дають число кластерів, що дорівнює $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск алгоритму класифікації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels, centroids = perform_kmeans_clustering(X_data_2d, n_clusters=k_opt)\n",
    "\n",
    "X_ceterod = centroids[:, 0]\n",
    "y_centroid = centroids[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Візуалізація результат роботи кластеризації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_with_convex_hulls(X_data_2d, cluster_labels, centroids, k_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Висновки\n",
    "\n",
    "Метод \"ліктя\" дає число ознак що дорівнює $2$, що підтверджується самим датасетом, де перша колонка є класом, що якої відносяться ознаки. Таких ознак там теж $2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет `mnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = pd.read_csv(\"data/mnist.csv\", header=None)\n",
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналіз даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mnist = mnist.iloc[:, 1:]\n",
    "y_mnist = mnist.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = mnist[0].unique()\n",
    "classes.sort()\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схоже, що датасет має $10$ кластерів. Ознаки мають характеризувати якусь цифру, можливо, це рукописні образи, як показано на рисунку.\n",
    "\n",
    "![Alt text](image/digit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Застосування PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) - це метод, що використовується для зменшення розмірності даних і виділення найбільш інформативних ознак. Основна ідея PCA полягає в проєктуванні багатовимірних даних на меншу кількість вимірів (головних компонентів), при цьому максимізується збереження дисперсії даних. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca_mnist = pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca_mnist[:, 0], X_pca_mnist[:, 1],\n",
    "c=y_mnist, edgecolor='none', alpha=0.5,\n",
    "cmap='tab10')\n",
    "plt.xlabel('component 1') \n",
    "plt.ylabel('component 2') \n",
    "plt.colorbar();\n",
    "\n",
    "plt.title(\"Visualization of clusters after PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дивлячись на діаграму розсіювання розфарбовану за класами, що наводяться в датасеті, не видно ніяких кластерів.\n",
    "\n",
    "Наразі, ці точки - це проекції кожної з точок даних уздовж напрямків максимальної дисперсії. По суті, ми знайшли оптимальні розтягнення і обертання в 784-вимірному просторі, що дають змогу побачити, який вигляд цифри мають у двох вимірах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Застосування методу \"ліктя\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### На даних без зниження розмірності"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jk_mnist = find_optimal_clusters(X_mnist, max_clusters=20, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут не можна надійно візуально визначити кількість кластерів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### На даних зі зниженням розмірності методом `PCA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jk_pca_mnist = find_optimal_clusters(X_pca_mnist, max_clusters=20, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визначаємо число кластерів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.diff(Jk_pca_mnist)\n",
    "D = diff[1:] / diff[:-1]\n",
    "plt.plot(np.arange(2, len(D) + 2), D)\n",
    "plt.xticks(np.arange(2, len(D) + 2))\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\frac{J_{k+1} - J_{k}}{J_{k} - J_{k-1}}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На цьому графіку, що побудований з використанням даних зі зниженням розмірності методом `PCA` видно, що число кластерів дорівнює $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_opt = np.argmin(D) + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автоматичне знаходження кластерів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kelbow_pca_mnist = kelbow_visualizer(\n",
    "    KMeans(random_state=random_state, n_init=\"auto\"),\n",
    "    X_pca_mnist,\n",
    "    k=(1, 21),\n",
    "    timings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут цікаво, що число кластерів $k = 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск алгоритму класифікації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels, centroids = perform_kmeans_clustering(X_pca_mnist, n_clusters=k_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_with_convex_hulls(X_pca_mnist, cluster_labels, centroids, k_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Висновки\n",
    "\n",
    "Метод \"ліктя\" після зниження розмірності (використання `PCA`) дає значення числа кластерів 3. Як це корелює з 10-ма класами (з 10-ма цифрами) не ясно.\n",
    "\n",
    "Крім того, я помітив, що мінімальне значення сильно залежить від `random_state`. При зміні цього знячення число властерів може бути 12 і 13. Що влизько до значення 10.\n",
    "\n",
    "Методами бібліотеки `yellowbrick` визначається $k = 5$, при чому, я не помітив, щоб це число залежало від `random_state`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
